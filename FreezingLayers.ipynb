{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FreezingLayers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc0mEzkyvjJy"
      },
      "source": [
        "# Install packages and import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBZhDmB75GZv"
      },
      "source": [
        "!pip install torchinfo datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQSy4E00iKam"
      },
      "source": [
        "pip install transformers==3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmcqprDSvpn_"
      },
      "source": [
        "import datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "import ipywidgets as widgets\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from collections import defaultdict\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrs5Ku_nEpmA"
      },
      "source": [
        "random_seed = 42\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCAUg2jQD0a_"
      },
      "source": [
        "# Data source, load datasets from huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Qn3Uivg2TS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7fV0XN-53bV"
      },
      "source": [
        "def get_dataset(dataset_name: str = None, path_name: str = None):\n",
        "    assert dataset_name is True or dataset_name is not None, \"Dataset Name is required\"\n",
        "    assert dataset_name in datasets_list, \"Invalid dataset name. Full list is \\n\"+str(datasets_list)\n",
        "    dataset_name=dataset_name\n",
        "    path_name=path_name\n",
        "    dataset_full = datasets.load_dataset(dataset_name,path_name)\n",
        "    \n",
        "    return dataset_full, list(dataset_full.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWmFwNzbznAl"
      },
      "source": [
        "import datasets\n",
        "datasets_list=datasets.list_datasets()\n",
        "print(\"List of datasets\")\n",
        "dataset_name_widget=widgets.Dropdown(\n",
        "    options=datasets_list,\n",
        ")\n",
        "display(dataset_name_widget)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uXsHrmT3kpF"
      },
      "source": [
        "dataset_name=dataset_name_widget.value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ22nkn4z0hs"
      },
      "source": [
        "config_name=None\n",
        "try:\n",
        "    dataset, splits=get_dataset(dataset_name)\n",
        "except ValueError as e:\n",
        "    print(\"Select a configuration\")\n",
        "    err=str(e)\n",
        "    config_list=err[err.index('[')+1:err.index(']')]\n",
        "    config_list=[c.strip()[1:-1] for c in config_list.split(',')]\n",
        "    config_list_widget=widgets.Dropdown(\n",
        "        options=config_list,\n",
        "    )\n",
        "    display(config_list_widget)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMr8rg_O48pp"
      },
      "source": [
        "config_name=config_list_widget.value \n",
        "dataset, splits=get_dataset(dataset_name, config_name)\n",
        "\n",
        "print(\"Available splits are:\",splits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBamO_Do9DQ9"
      },
      "source": [
        "#Change split names as needed\n",
        "train_data=dataset['train']\n",
        "val_data=dataset['validation']\n",
        "test_data=dataset['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF5Z_RxcIjLG"
      },
      "source": [
        "### Convert data to required format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb7wP0WMtlsX"
      },
      "source": [
        "def encode(examples):\n",
        "    return tokenizer(examples[key_1], examples[key_2], truncation=\"longest_first\", padding='max_length', max_length=100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wD0hdbkFC7o"
      },
      "source": [
        "def convert_data(batch_size: int, field_names: list, data):\n",
        "    \n",
        "    return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idwpiauXpmqK"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "batch_size=128\n",
        "key_1, key_2=\"sentence1\",\"sentence2\" #These are the keys to encode in the data, using the encode() function\n",
        "train_data=train_data.map(encode, batched=True)\n",
        "train_data=train_data.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
        "train_data.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
        "train_data=train_data.rename_column(\"attention_mask\",\"mask\")\n",
        "train_data=train_data.rename_column(\"input_ids\", \"sent_id\")\n",
        "train_data=train_data.remove_columns([\"idx\",key_1,key_2])\n",
        "train_dataloader=torch.utils.data.DataLoader(train_data, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi1NsPk-jgl5"
      },
      "source": [
        "val_data=val_data.map(encode, batched=True)\n",
        "val_data=val_data.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
        "val_data.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
        "val_data=val_data.rename_column(\"attention_mask\",\"mask\")\n",
        "val_data=val_data.rename_column(\"input_ids\", \"sent_id\")\n",
        "val_data=val_data.remove_columns([\"idx\",key_1,key_2])\n",
        "val_dataloader=torch.utils.data.DataLoader(val_data, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxD6wlq7EA3e"
      },
      "source": [
        "# Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI5fYjmUXmYY"
      },
      "source": [
        "## Get model from huggingface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiHb7l9VBO-C"
      },
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "base_model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw0u8TPDBzeh"
      },
      "source": [
        "from torchinfo import summary\n",
        "summary(base_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqFweuksEm4l"
      },
      "source": [
        "## Freeze layers of the Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVUE-qZe8xLT"
      },
      "source": [
        "def freeze_model(model, freeze_layer_count: int = 0):\n",
        "    '''\n",
        "    Set freeze_layer_count to -1 if you want \n",
        "    just the embedding layers to be frozen\n",
        "    '''\n",
        "    if freeze_layer_count:\n",
        "        #Freeze embeddings layers\n",
        "        for param in model.embeddings.parameters():\n",
        "            param.requires_grad=False\n",
        "    \n",
        "        if freeze_layer_count!=-1:\n",
        "            for layer in model.encoder.layer[:freeze_layer_count]:\n",
        "                    for param in layer.parameters():\n",
        "                        param.requires_grad = False\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1zB_I8pEgY8"
      },
      "source": [
        "## Define model architecture on top of base transformer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9zC7iZnNnP-"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "\n",
        "    def __init__(self, base_model, num_classes):\n",
        "      \n",
        "      super(Classifier, self).__init__()\n",
        "\n",
        "      self.bert = base_model\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.relu =  nn.ReLU()\n",
        "      self.fc1 = nn.Linear(768,256)\n",
        "      self.fc2 = nn.Linear(256,num_classes)\n",
        "      self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)     \n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.fc2(x)      \n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl2rQ783rA4-"
      },
      "source": [
        "num_classes=train_data.features['label'].num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XMxU5slESwW"
      },
      "source": [
        "# Training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBHGlHhwvYiZ"
      },
      "source": [
        "#function for training the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  total_loss=0.0\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch ={k: v.to(device) for k, v in batch.items()}\n",
        "    labels, mask, sent_id = batch['labels'], batch['mask'], batch['sent_id']\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = criterion(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL3xSwmovvZK"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(tqdm(val_dataloader)):\n",
        "    \n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch ={k: v.to(device) for k, v in batch.items()}\n",
        "    labels, mask, sent_id = batch['labels'], batch['mask'], batch['sent_id']\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = criterion(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4UqxUls1Ede"
      },
      "source": [
        "#Function to combine training and validation loops\n",
        "def compute():\n",
        "    # set initial loss to infinite\n",
        "    best_valid_loss = float('inf')\n",
        "    training_metrics, validation_metrics=defaultdict(list), defaultdict(list)\n",
        "    times=defaultdict(float)\n",
        "    # empty lists to store training and validation loss of each epoch\n",
        "    train_losses=[]\n",
        "    valid_losses=[]\n",
        "    torch.manual_seed(random_seed)\n",
        "    train_labels=train_data['labels']\n",
        "    valid_labels=val_data['labels']\n",
        "    #for each epoch\n",
        "    for epoch in range(epochs):\n",
        "        start_time=time.perf_counter()\n",
        "        print('\\nEpoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "        \n",
        "        #train model\n",
        "        train_loss, train_preds = train() \n",
        "        train_preds= torch.argmax(torch.tensor(train_preds),dim=1)\n",
        "        train_acc, train_f1=accuracy_score(train_labels, train_preds)*100, f1_score(train_labels, train_preds)*100\n",
        "        print(\"\\nTraining metrics \\n Accuracy: {:.3f}, F-1 Score: {:.3f}\".format(train_acc, train_f1))\n",
        "        #evaluate model\n",
        "        valid_loss, valid_preds = evaluate()\n",
        "        valid_preds= torch.argmax(torch.tensor(valid_preds),dim=1)\n",
        "        valid_acc, valid_f1=accuracy_score(valid_labels, valid_preds)*100, f1_score(valid_labels, valid_preds)*100\n",
        "        print(\"\\nValidation metrics \\n Accuracy: {:.3f}, F-1 Score: {:.3f}\".format(valid_acc, valid_f1))\n",
        "        end_time=time.perf_counter()\n",
        "        times[epoch]=end_time-start_time\n",
        "        #save the best model\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "        \n",
        "        # append training and validation loss\n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "        training_metrics[epoch]=[train_acc, train_f1]\n",
        "        validation_metrics[epoch]=[valid_acc, valid_f1]\n",
        "\n",
        "    return training_metrics, validation_metrics, times"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlPUeeRVEY4N"
      },
      "source": [
        "# Testing and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiAjl4dUcnCg"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "metrics_per_run=defaultdict(dict)\n",
        "for n in range(2,6,2):\n",
        "    base_model=freeze_model(base_model, freeze_layer_count=n)\n",
        "    model=Classifier(base_model,num_classes=num_classes)\n",
        "    model.to(device)\n",
        "    lr=1e-4\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = lr, weight_decay=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print(\"\\nTraining with {} layers frozen\".format(n))\n",
        "    metrics_per_run[n]=compute()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rrklqw2p1kMC"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "fig,ax=plt.subplots()\n",
        "for key in sorted(list(metrics_per_run.keys())):\n",
        "    accs=metrics_per_run[key][0]\n",
        "    f1s=metrics_per_run[key][1]\n",
        "    ax.set_xlabel(\"Epochs\")\n",
        "    # ax.set_ylabel(\"%\")\n",
        "    # ax.plot(range(epochs), [accs[i][0]for i in range(epochs)], label='T_A_'+str(key))\n",
        "    ax.plot(range(epochs), [accs[i][1]for i in range(epochs)], label='V_A_'+str(key))\n",
        "    # ax.plot(range(epochs), [f1s[i][0]for i in range(epochs)], label='T_F1_'+str(key))\n",
        "    ax.plot(range(epochs), [f1s[i][1]for i in range(epochs)], label='V_F1_'+str(key))\n",
        "    plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TglPTV2H0iya"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}